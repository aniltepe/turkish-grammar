{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 369,
      "metadata": {
        "id": "3WbK8SNN5vPY"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "import json\n",
        "import types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 370,
      "metadata": {},
      "outputs": [],
      "source": [
        "f = open('rules.json')\n",
        "RULES = json.load(f)\n",
        "f = open('dictionary.json')\n",
        "DICTIONARY = json.load(f)\n",
        "f = open('sequences.json')\n",
        "SEQUENCES = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 371,
      "metadata": {
        "id": "cDnLHUCu56fe"
      },
      "outputs": [],
      "source": [
        "class _l(Enum): # letter\n",
        "  SESLI = 1\n",
        "  SESSIZ = 2\n",
        "  KALIN = 3\n",
        "  INCE = 4\n",
        "  DUZ = 5\n",
        "  YUVARLAK = 6\n",
        "  GENIS = 7\n",
        "  DAR = 8\n",
        "  SERT = 9\n",
        "  YUMUSAK = 10\n",
        "  SUREKLI = 11\n",
        "  SUREKSIZ = 12\n",
        "  AKICI = 13\n",
        "  SIZICI = 14\n",
        "  KAYNASTIRMA = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 372,
      "metadata": {
        "id": "dJlO3hqJO_KF"
      },
      "outputs": [],
      "source": [
        "class _w(Enum): # word\n",
        "  ISIM = 1\n",
        "  FIIL = 2\n",
        "  SIFAT = 3\n",
        "  ZARF = 4\n",
        "  ZAMIR = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 373,
      "metadata": {
        "id": "Na1AkqLrl65v"
      },
      "outputs": [],
      "source": [
        "class _t(Enum): # tense\n",
        "  GENIS = 1\n",
        "  SIMDIKI_YOR = 2\n",
        "  SIMDIKI_MAKTA = 3\n",
        "  BILINENGECMIS = 4\n",
        "  DUYULANGECMIS = 5\n",
        "  GELECEK = 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 374,
      "metadata": {
        "id": "JL0ZQ2scfTAb"
      },
      "outputs": [],
      "source": [
        "class _p(Enum): # pronoun\n",
        "  BIRINCITEKIL = 1\n",
        "  IKINCITEKIL = 2\n",
        "  UCUNCUTEKIL = 3\n",
        "  BIRINCICOGUL = 4\n",
        "  IKINCICOGUL = 5\n",
        "  UCUNCUCOGUL = 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 375,
      "metadata": {},
      "outputs": [],
      "source": [
        "class _c(Enum): # compound\n",
        "    BIRLESIKFIIL = 1\n",
        "    YETERLILIK = 2\n",
        "    TEZLIK = 3\n",
        "    SUREKLILIK_DUR = 4\n",
        "    SUREKLILIK_KAL = 5\n",
        "    SUREKLILIK_GEL = 6\n",
        "    YAKLASMA = 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 376,
      "metadata": {},
      "outputs": [],
      "source": [
        "class _o(Enum):\n",
        "    OLUMSUZLUK = 1\n",
        "    GEREKLILIK = 2\n",
        "    ISTEKKIPI = 3\n",
        "    EMIRKIPI = 4\n",
        "    SARTKIPI = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 377,
      "metadata": {
        "id": "QJIDlPjv9ube"
      },
      "outputs": [],
      "source": [
        "_a = {\n",
        "  'a': {'index': 1, 'lower': 'a', 'upper': 'A', 'tags': [_l.SESLI, _l.KALIN, _l.DUZ, _l.GENIS] },\n",
        "  'b': {'index': 2, 'lower': 'b', 'upper': 'B', 'tags': [_l.SESSIZ, _l.YUMUSAK, _l.SUREKSIZ] },\n",
        "  'c': {'index': 3, 'lower': 'c', 'upper': 'C', 'tags': [_l.SESSIZ, _l.YUMUSAK, _l.SUREKSIZ] },\n",
        "  'ç': {'index': 4, 'lower': 'ç', 'upper': 'Ç', 'tags': [_l.SESSIZ, _l.SERT, _l.SUREKSIZ] },\n",
        "  'd': {'index': 5, 'lower': 'd', 'upper': 'D', 'tags': [_l.SESSIZ, _l.YUMUSAK, _l.SUREKSIZ] },\n",
        "  'e': {'index': 6, 'lower': 'e', 'upper': 'E', 'tags': [_l.SESLI, _l.INCE, _l.DUZ, _l.GENIS] },\n",
        "  'f': {'index': 7, 'lower': 'f', 'upper': 'F', 'tags': [_l.SESSIZ, _l.SERT, _l.SUREKLI] },\n",
        "  'g': {'index': 8, 'lower': 'g', 'upper': 'G', 'tags': [_l.SESSIZ, _l.YUMUSAK, _l.SUREKSIZ] },\n",
        "  'ğ': {'index': 9, 'lower': 'ğ', 'upper': 'Ğ', 'tags': [_l.SESSIZ, _l.YUMUSAK, _l.SUREKLI] },\n",
        "  'h': {'index': 10, 'lower': 'h', 'upper': 'H', 'tags': [_l.SESSIZ, _l.SERT, _l.SUREKLI] },\n",
        "  'ı': {'index': 11, 'lower': 'ı', 'upper': 'I', 'tags': [_l.SESLI, _l.KALIN, _l.DUZ, _l.DAR] },\n",
        "  'i': {'index': 12, 'lower': 'i', 'upper': 'İ', 'tags': [_l.SESLI, _l.INCE, _l.DUZ, _l.DAR] },\n",
        "  'j': {'index': 13, 'lower': 'j', 'upper': 'J', 'tags': [_l.SESSIZ, _l.YUMUSAK, _l.SUREKLI] },\n",
        "  'k': {'index': 14, 'lower': 'k', 'upper': 'K', 'tags': [_l.SESSIZ, _l.SERT, _l.SUREKSIZ] },\n",
        "  'l': {'index': 15, 'lower': 'l', 'upper': 'L', 'tags': [_l.SESSIZ, _l.YUMUSAK, _l.SUREKLI] },\n",
        "  'm': {'index': 16, 'lower': 'm', 'upper': 'M', 'tags': [_l.SESSIZ, _l.YUMUSAK, _l.SUREKLI] },\n",
        "  'n': {'index': 17, 'lower': 'n', 'upper': 'N', 'tags': [_l.SESSIZ, _l.YUMUSAK, _l.SUREKLI, _l.KAYNASTIRMA] },\n",
        "  'o': {'index': 18, 'lower': 'o', 'upper': 'O', 'tags': [_l.SESLI, _l.KALIN, _l.YUVARLAK, _l.GENIS] },\n",
        "  'ö': {'index': 19, 'lower': 'ö', 'upper': 'Ö', 'tags': [_l.SESLI, _l.INCE, _l.YUVARLAK, _l.GENIS] },\n",
        "  'p': {'index': 20, 'lower': 'p', 'upper': 'P', 'tags': [_l.SESSIZ, _l.SERT, _l.SUREKSIZ] },\n",
        "  'r': {'index': 21, 'lower': 'r', 'upper': 'R', 'tags': [_l.SESSIZ, _l.YUMUSAK, _l.SUREKLI] },\n",
        "  's': {'index': 22, 'lower': 's', 'upper': 'S', 'tags': [_l.SESSIZ, _l.SERT, _l.SUREKLI, _l.KAYNASTIRMA] },\n",
        "  'ş': {'index': 23, 'lower': 'ş', 'upper': 'Ş', 'tags': [_l.SESSIZ, _l.SERT, _l.SUREKLI, _l.KAYNASTIRMA] },\n",
        "  't': {'index': 24, 'lower': 't', 'upper': 'T', 'tags': [_l.SESSIZ, _l.SERT, _l.SUREKSIZ] },\n",
        "  'u': {'index': 25, 'lower': 'u', 'upper': 'U', 'tags': [_l.SESLI, _l.KALIN, _l.YUVARLAK, _l.DAR] },\n",
        "  'ü': {'index': 26, 'lower': 'ü', 'upper': 'Ü', 'tags': [_l.SESLI, _l.INCE, _l.YUVARLAK, _l.DAR] },\n",
        "  'v': {'index': 27, 'lower': 'v', 'upper': 'V', 'tags': [_l.SESSIZ, _l.YUMUSAK, _l.SUREKLI] },\n",
        "  'y': {'index': 28, 'lower': 'y', 'upper': 'Y', 'tags': [_l.SESSIZ, _l.YUMUSAK, _l.SUREKLI, _l.KAYNASTIRMA] },\n",
        "  'z': {'index': 29, 'lower': 'z', 'upper': 'Z', 'tags': [_l.SESSIZ, _l.YUMUSAK, _l.SUREKLI] }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 378,
      "metadata": {},
      "outputs": [],
      "source": [
        "def FIND_LETTERS(tags):\n",
        "  return [_a[l] for l in _a if all(t in _a[l]['tags'] for t in tags)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 379,
      "metadata": {},
      "outputs": [],
      "source": [
        "def CHECK_LETTER(letter, tags):\n",
        "    return all(t in letter['tags'] for t in tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 380,
      "metadata": {},
      "outputs": [],
      "source": [
        "def REPLACE_TAGS(tags, old, new):\n",
        "    return [t if t != old else new for t in tags]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 381,
      "metadata": {
        "id": "H2ZUCm7F2Fvx"
      },
      "outputs": [],
      "source": [
        "class Word:\n",
        "  def __init__(self, word, type=None):\n",
        "    self.root = word\n",
        "    if type == _w.FIIL:\n",
        "      self.root = word[:-3]\n",
        "    self.letters, self.vowels, self.consonants = self.set_letters(self.root)\n",
        "    self.default = word\n",
        "    self.type = type\n",
        "\n",
        "    self.applied = []\n",
        "    self.ambiguous = False\n",
        "\n",
        "    self.next_word = None\n",
        "    self.prev_word = None\n",
        "    self.punct_after = None\n",
        "  \n",
        "  def word(self, type='lower'):\n",
        "    w = (''.join([l['upper'] for l in self.letters]) if type == 'upper'\n",
        "         else ''.join([l['upper'] if i == 0 else l['lower'] for i, l in enumerate(self.letters)]) if type == 'pascal' \n",
        "         else ''.join([l['lower'] for l in self.letters]))\n",
        "    return w\n",
        "\n",
        "  def append(self, suffix, code, prev_suffix=None):\n",
        "    # print(''.join([l['lower'] for l in suffix]))\n",
        "    self._suffix = (suffix, code)\n",
        "    if len(suffix) > 0:\n",
        "      ns = types.SimpleNamespace()\n",
        "      setattr(ns, 'name', '_APPEND')\n",
        "      self.apply_rule(ns)\n",
        "      count = len(self.letters)\n",
        "      self.letters.extend(self._suffix[0])\n",
        "      self.vowels.extend([dict(l, **{'w_idx': count + i}) for i, l in enumerate(self._suffix[0]) if _l.SESLI in l['tags']])\n",
        "      self.consonants.extend([dict(l, **{'w_idx': count + i}) for i, l in enumerate(self._suffix[0]) if _l.SESSIZ in l['tags']])\n",
        "    self.applied.append({'code': code, 'type': 'append', 'field1': self._suffix[0]})\n",
        "    self._suffix = prev_suffix\n",
        "    \n",
        "  def replace(self, idx, new, code):\n",
        "    old = self.letters[idx]\n",
        "    self.letters = [l if i != idx else new for i, l in enumerate(self.letters)]\n",
        "    if _l.SESLI in new['tags']:\n",
        "      self.vowels = [l if l['w_idx'] != idx else dict(new, **{'w_idx': idx}) for l in self.vowels]\n",
        "    elif _l.SESSIZ in new['tags']:\n",
        "      self.consonants = [l if l['w_idx'] != idx else dict(new, **{'w_idx': idx}) for l in self.consonants]\n",
        "    self.applied.append({'code': code, 'type': 'replace', 'field1': (old, idx), 'field2': new})\n",
        "  \n",
        "  def replace_suffix(self, idx, new, code):\n",
        "    old = self._suffix[0][idx]\n",
        "    self._suffix = ([l if i != idx else new for i, l in enumerate(self._suffix[0])], self._suffix[1])\n",
        "    self.applied.append({'code': code, 'type': 'replace', 'field1': (old, len(self.letters) + idx), 'field2': new})\n",
        "\n",
        "  def apply_rule(self, type):\n",
        "    rules = [r for r in RULES if r['type'] == type.name]\n",
        "    var_match = {\n",
        "      'self': self,\n",
        "      '_l': _l,\n",
        "      '_w': _w,\n",
        "      '_a': _a,\n",
        "      'CHECK_LETTER': CHECK_LETTER,\n",
        "      'FIND_LETTERS': FIND_LETTERS,\n",
        "      'REPLACE_TAGS': REPLACE_TAGS\n",
        "    }\n",
        "    for rule in rules:\n",
        "      if 'if' not in rule or eval(' and '.join(rule['if']), var_match):\n",
        "        # print(rule['id'])\n",
        "        for do in rule['do']:\n",
        "          eval(do, var_match)\n",
        "        break\n",
        "\n",
        "  def apply_sequences(self):\n",
        "    results = []\n",
        "    var_match = {'self': self, '_w': _w, '_t': _t, '_c': _c, '_p': _p, '_o': _o}\n",
        "    for SEQUENCE in SEQUENCES:\n",
        "      if eval(SEQUENCE['if'], var_match):\n",
        "        w_root = self.clone()\n",
        "        word_build = [[w_root]]\n",
        "        for rule in SEQUENCE['seq']:\n",
        "          possibles = []\n",
        "          obj = eval(rule, var_match)\n",
        "          for last_possible in word_build[-1]:\n",
        "            if type(obj) == type(Enum):\n",
        "              for i in obj:\n",
        "                w_new = last_possible.clone()\n",
        "                w_new.apply_rule(i)\n",
        "                possibles.append(w_new)\n",
        "            elif type(type(obj)) == type(Enum):\n",
        "              w_new = last_possible.clone()\n",
        "              w_new.apply_rule(obj)\n",
        "              possibles.append(w_new)\n",
        "          word_build.append(possibles)\n",
        "        for all_possible in word_build[-1]:\n",
        "          results.append((all_possible.word(), [a['code'] for a in all_possible.applied]))\n",
        "    return results\n",
        "\n",
        "  def check_applied(self, codes, type, idx=None):\n",
        "    if idx == None:\n",
        "      return (all(t in [a['code'] for a in self.applied] for t in codes) if type == 'allin'\n",
        "              else all(t not in [a['code'] for a in self.applied] for t in codes) if type == 'allnotin'\n",
        "              else any(t in [a['code'] for a in self.applied] for t in codes) if type == 'anyin'\n",
        "              else any(t not in [a['code'] for a in self.applied] for t in codes) if type == 'anynotin'\n",
        "              else not(any(t in [a['code'] for a in self.applied] for t in codes)) if type == 'notanyin'\n",
        "              else None)\n",
        "    if (idx < 0 and idx * -1 > len(self.applied)) or (idx >= 0 and idx >= len(self.applied)):\n",
        "      return True if type == 'notin' else False if type == 'in' else None\n",
        "    return (self.applied[idx]['code'] in codes if type == 'in'\n",
        "            else self.applied[idx]['code'] not in codes if type == 'notin'\n",
        "            else None)\n",
        "\n",
        "  def clone(self):\n",
        "    w_new = Word('')\n",
        "    w_new.letters = self.letters[:]\n",
        "    w_new.vowels = self.vowels[:]\n",
        "    w_new.consonants = self.consonants[:]\n",
        "    w_new.applied = self.applied[:]\n",
        "    w_new.type = self.type\n",
        "    w_new.default = self.default\n",
        "    w_new.root = self.root\n",
        "    return w_new\n",
        "  \n",
        "  @staticmethod\n",
        "  def set_letters(word):\n",
        "    letters = [_a[l] for l in Word.lower(word)]\n",
        "    vowels = [dict(_a[l], **{'w_idx': i}) for i, l in enumerate(Word.lower(word)) if _l.SESLI in _a[l]['tags']]\n",
        "    consonants = [dict(_a[l], **{'w_idx': i}) for i, l in enumerate(Word.lower(word)) if _l.SESSIZ in _a[l]['tags']]\n",
        "    return letters, vowels, consonants\n",
        "  \n",
        "  @staticmethod\n",
        "  def lower(word):\n",
        "    return word.replace('İ','i').replace('I','ı').lower()\n",
        "  \n",
        "  @staticmethod\n",
        "  def upper(word):\n",
        "    return word.replace('i','İ').upper()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 382,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Sentence:\n",
        "    def __init__(self, sentence):\n",
        "        self.words = [Word(w) for w in sentence.split(' ')]\n",
        "        self.punctuation = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 383,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEST_PARAGRAPH = 'Çalışma hayatına dahil edilen çocuklar oyun oynama, öğrenme, hayal kurma ve dinlenme gibi temel ihtiyaçlardan mahrum bırakılmaktadır. Çalışan çocuklar, çocukluklarını yaşayamamanın ötesinde ağır ve sağlıksız çalışma koşulları nedeniyle bedensel, ruhsal ve sosyal anlamda zarar görme tehlikesiyle karşı karşıya kalmaktadır. Aslında hemen hemen her evde belirli bir yaşın üzerindeki çocuklar ev işlerinde çalışırlar. Bu çalışma kimi zaman küçük bir kardeşin bakımı veya evin derlenip toparlanması, kimi zaman da tarla ve bahçe işlerine yardım etme şeklinde olabilir. Bunlar genellikle yarı zamanlı, belirli bir zamanlaması olmayan, o andaki ihtiyaca göre şekillenen geçici işlerdir. Oysa çalışan çocuk kavramı ile çocuğun kendi evinin dışında başkasının işinde veya kendi ailesinin işinde üretime doğrudan katkı sağlayacak şekilde çalışması kastedilmektedir.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 384,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEST_SENTENCE = 'Bilirsin.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 385,
      "metadata": {},
      "outputs": [],
      "source": [
        "w1 = Word('olmak', _w.FIIL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 386,
      "metadata": {},
      "outputs": [],
      "source": [
        "possibles = w1.apply_sequences()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 387,
      "metadata": {
        "id": "qc74hkRuQ388"
      },
      "outputs": [],
      "source": [
        "VERBS = [w['word'] for w in DICTIONARY if w['type'] == _w.FIIL.value]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 388,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "açmak açabiliyorum\n",
            "almak alabiliyorum\n",
            "anlamak anlayabiliyorum\n",
            "aramak arayabiliyorum\n",
            "atmak atabiliyorum\n",
            "ayırmak ayırabiliyorum\n",
            "bakmak bakabiliyorum\n",
            "başlamak başlayabiliyorum\n",
            "beklemek bekleyebiliyorum\n",
            "bilmek bilebiliyorum\n",
            "bitmek bitebiliyorum\n",
            "bölmek bölebiliyorum\n",
            "bulmak bulabiliyorum\n",
            "çalışmak çalışabiliyorum\n",
            "çarpmak çarpabiliyorum\n",
            "çevirmek çevirebiliyorum\n",
            "çıkarmak çıkarabiliyorum\n",
            "çıkmak çıkabiliyorum\n",
            "değişmek değişebiliyorum\n",
            "demek diyebiliyorum\n",
            "dinlemek dinleyebiliyorum\n",
            "doğmak doğabiliyorum\n",
            "dönmek dönebiliyorum\n",
            "durmak durabiliyorum\n",
            "duymak duyabiliyorum\n",
            "düşünmek düşünebiliyorum\n",
            "eklemek ekleyebiliyorum\n",
            "ekmek ekebiliyorum\n",
            "etmek edebiliyorum\n",
            "evlenmek evlenebiliyorum\n",
            "geçmek geçebiliyorum\n",
            "gelmek gelebiliyorum\n",
            "girmek girebiliyorum\n",
            "gitmek gidebiliyorum\n",
            "görmek görebiliyorum\n",
            "gülmek gülebiliyorum\n",
            "hazırlamak hazırlayabiliyorum\n",
            "hissetmek hissedebiliyorum\n",
            "içmek içebiliyorum\n",
            "inanmak inanabiliyorum\n",
            "istemek isteyebiliyorum\n",
            "izlemek izleyebiliyorum\n",
            "kalmak kalabiliyorum\n",
            "kapatmak kapatabiliyorum\n",
            "katılmak katılabiliyorum\n",
            "kazanmak kazanabiliyorum\n",
            "kesmek kesebiliyorum\n",
            "konuşmak konuşabiliyorum\n",
            "kullanmak kullanabiliyorum\n",
            "kurmak kurabiliyorum\n",
            "kurumak kuruyabiliyorum\n",
            "okumak okuyabiliyorum\n",
            "olmak olabiliyorum\n",
            "oynamak oynayabiliyorum\n",
            "öğrenmek öğrenebiliyorum\n",
            "ölmek ölebiliyorum\n",
            "sağlamak sağlayabiliyorum\n",
            "saymak sayabiliyorum\n",
            "seçmek seçebiliyorum\n",
            "sevmek sevebiliyorum\n",
            "seyretmek seyredebiliyorum\n",
            "sormak sorabiliyorum\n",
            "sürmek sürebiliyorum\n",
            "taşımak taşıyabiliyorum\n",
            "toplamak toplayabiliyorum\n",
            "toplanmak toplanabiliyorum\n",
            "tutmak tutabiliyorum\n",
            "ulaşmak ulaşabiliyorum\n",
            "unutmak unutabiliyorum\n",
            "uygulamak uygulayabiliyorum\n",
            "uyumak uyuyabiliyorum\n",
            "üretmek üretebiliyorum\n",
            "varmak varabiliyorum\n",
            "vermek verebiliyorum\n",
            "vurmak vurabiliyorum\n",
            "yakmak yakabiliyorum\n",
            "yapmak yapabiliyorum\n",
            "yaratmak yaratabiliyorum\n",
            "yaşamak yaşayabiliyorum\n",
            "yazmak yazabiliyorum\n",
            "yemek yiyebiliyorum\n",
            "yürümek yürüyebiliyorum\n"
          ]
        }
      ],
      "source": [
        "for verb in VERBS:\n",
        "  w = Word(verb, _w.FIIL)\n",
        "  # w.apply_rule(_o.OLUMSUZLUK)\n",
        "  w.apply_rule(_c.YETERLILIK)\n",
        "  w.apply_rule(_t.SIMDIKI_YOR)\n",
        "  w.apply_rule(_p.BIRINCITEKIL)\n",
        "  print(verb, w.word())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "virtualenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
